{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aalargen/DeepDive/blob/main/DeepDiveTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeepDive: Read Out *Anything* from Deep Nets"
      ],
      "metadata": {
        "id": "5uLW4cEeG5Kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[*DeepDive*](https://github.com/ColinConwell/DeepDive) is a toolkit for comprehensive and highly efficient deep net feature extraction, in concert with tools for representational similarity analysis and linear probes. In this demo, we'll look at how we can use effectively the same analytic approach to map DNN responses to human fMRI data in an encoding model, and to behavioral categorization in a decoding model."
      ],
      "metadata": {
        "id": "oPWmkOQcHIma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind this is an active work in progress. If you have any issues, please refer to the associated [Github](https://github.com/ColinConwell/DeepNSD) repo(s)."
      ],
      "metadata": {
        "id": "FFz7w9xdGtmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Deep Dive for Encoding: NSD (Human fMRI) Case Study\n",
        "\n"
      ],
      "metadata": {
        "id": "TKT71osCJsz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a first case study, let' see how we can extract the features from across the layers of a CLIP model, and use those features to predict hemodynamic activity in the human brain."
      ],
      "metadata": {
        "id": "1kDH1jPlkPth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyMY_7_B-SJb"
      },
      "outputs": [],
      "source": [
        "# first things first, we'll clone the Github repo...\n",
        "!git clone https://github.com/ColinConwell/DeepDive\n",
        "!git clone https://github.com/ColinConwell/DeepNSD\n",
        "!cp -r DeepDive/deepdive DeepNSD/model_opts #toolbox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd DeepNSD"
      ],
      "metadata": {
        "id": "oWgHS6esNX_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The brain data we'll be predicting today is the fMRI activity of 1 human subject across 16 visual cortical regions, collected in response to 1000 natural scenes from the Microsoft Coco (2017) image set.\n",
        "\n",
        "We load and manipulate this data as a python class, with native functions built-in that allow for visualizing stimulus examples, grouping the neural responses by various kinds of metadata, and quickly constructing RDMs across arbitrary levels of organization."
      ],
      "metadata": {
        "id": "DnIOfMZp01zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#functions for loading and manipulating the neural data\n",
        "from neural_data import *"
      ],
      "metadata": {
        "id": "ZAhYFYJH-XjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here, we instantiate the neural_data class\n",
        "benchmark = NaturalScenesDataset()"
      ],
      "metadata": {
        "id": "1qD7lL-t-kxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main data of the **benchmark()** class is organized into 3 main dataframes:\n",
        "*   **benchmark.response_data** contains the average brain activity per voxel (row) per stimulus (column); the index is the *voxel_id*\n",
        "*   **benchmark.metadata** contains metadata (columns) about each voxel (row), for example its ROI; the index is the *voxel_id*\n",
        "*   **benchmark.stimulus_data** contains information (columns) about each stimulus (row), including its relative path\n",
        "\n"
      ],
      "metadata": {
        "id": "A8FAGxO62jx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here, we see the shapes of these 3 main dataframes\n",
        "# there are 8461 voxels in our dataset\n",
        "# ...responding to 1000 images.\n",
        "print(benchmark.response_data.shape,\n",
        "      benchmark.metadata.shape,\n",
        "      benchmark.stimulus_data.shape)"
      ],
      "metadata": {
        "id": "kDGlNCNR4neo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here's a sample stimulus (stimulus 0)\n",
        "benchmark.view_sample_stimulus(0)"
      ],
      "metadata": {
        "id": "KNROtiPa-wMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructing target RDMs in this case requires simply specifying a set of grouping_variables (*group_vars*) -- whatever is available in the benchmark metadata. In this case, we'll create one RDM per ROI per subject."
      ],
      "metadata": {
        "id": "sCLWyBWV5beb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here are the various grouping_variables we could use from the metadata\n",
        "print(benchmark.metadata.columns.to_list())"
      ],
      "metadata": {
        "id": "LN0FNupN5MqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# while the neural_data loads with a default set of RDMs,\n",
        "# here we'll overwrite these with our own\n",
        "benchmark.rdms = benchmark.get_rdms(group_vars = ['roi_name', 'subj_id'])\n",
        "\n",
        "# note, we also want to save the indices associated with the responses\n",
        "# that constitute each RDM, so we can parse the predicted responses\n",
        "# in our neural encoding procedure (for voxel-encoding RSA)\n",
        "benchmark.rdm_indices = benchmark.get_rdm_indices(['roi_name','subj_id'])"
      ],
      "metadata": {
        "id": "BvqMrIAHCtAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having instantiated our neural data, we'll now 'instrumentalize' a deep neural network model to predict this data. In this example, we'll use a CLIP model, but there are a wide number of other options available in this repo."
      ],
      "metadata": {
        "id": "mEUFY42b7XFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# functions for loading models\n",
        "sys.path.append('model_opts')\n",
        "from model_options import *"
      ],
      "metadata": {
        "id": "xIhAaLEl_gWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here's a sample of the models we've preprocessed for easy use.\n",
        "list(get_model_options(model_source = 'timm').keys())[::21]"
      ],
      "metadata": {
        "id": "cbmfmCXCDaUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to use the clip model, we need to install CLIP\n",
        "!pip install -q git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "U6EvQLAd_hmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline requires a few things for the models we want to test. It requires some **metadata** (the name of the model, what it's been trained on, what class of model it is); a **call** for loading the model; and (perhaps most importantly of all) the appropriate **image transforms** that'll convert our stimulus images into the right format (and distribution) for our model.\n",
        "\n",
        "So long as you have a PyTorch model, a torchvision *transforms.Compose()* function, and a *model_string*, *model_name*, and *train_type*, you can use any model at your disposal, simply modifying the pipeline below without the *get_model_options()* function."
      ],
      "metadata": {
        "id": "ylVF28S88tQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_string = 'ViT-B/32_clip'\n",
        "model_option = get_model_options()[model_string]\n",
        "\n",
        "image_transforms = get_recommended_transforms(model_string)\n",
        "model = eval(model_option['call'])\n",
        "model = model.eval()\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()"
      ],
      "metadata": {
        "id": "q5-Zqm5W_t-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our neural data instantiated, and model instrumentalized, we now begin the process of feature extraction. To do this, we first select our target layers (though in reality, you'll usually want to test all layers). We then hook the features from these target layers as we pass batches of images through the model.  Then (for computational tractability), we reduce the dimensionality of these features (using in this case a technique called sparse random projection)."
      ],
      "metadata": {
        "id": "Zvs0ytFu-YsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tools for feature extraction\n",
        "sys.path.append('model_opts')\n",
        "from feature_extraction import *"
      ],
      "metadata": {
        "id": "1JJdUVz1ALQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stimulus_loader = DataLoader(StimulusSet(benchmark.stimulus_data.image_path, image_transforms), batch_size = 64)\n",
        "# this function creates a dataloader using the stimulus_data dataframe of our neural benchmark,\n",
        "# and the image_transforms we loaded when we instrumentalized our model."
      ],
      "metadata": {
        "id": "54CjSnKq_8gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here's a sample of the dataloader (with the transforms reversed)\n",
        "from model_opts_utils import *\n",
        "get_dataloader_sample(stimulus_loader, nrow = 8)"
      ],
      "metadata": {
        "id": "FDbJabYOAJU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get the names of the model layers we'll subselect from, we'll use a convenience function\n",
        "# because we're testing a vision transformer, we'll then extract every 5th linear layer.\n",
        "model_layers = get_empty_feature_maps(model, stimulus_loader, names_only = True)\n",
        "target_layers = [layer for layer in model_layers if 'Linear' in layer\n",
        "                 and int(layer.split('-')[1]) % 5 == 1] + ['Vision-Transformer-1']"
      ],
      "metadata": {
        "id": "aGKCgu1J_uda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once we've selected our target layers, we pass these, our model, and our dataloader through a convenience function:\n",
        "feature_maps = get_all_feature_maps(model, stimulus_loader, layers_to_retain = target_layers)"
      ],
      "metadata": {
        "id": "QV4O1H9bAUQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here are the dimensionalities of our extracted features\n",
        "# obviously, these take up a lot of RAM!\n",
        "for feature_map in feature_maps:\n",
        "  print(feature_map, feature_maps[feature_map].shape)"
      ],
      "metadata": {
        "id": "Y0LxoJaXD-4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tools for dimensionality reduction\n",
        "sys.path.append('model_opts')\n",
        "from feature_reduction import *"
      ],
      "metadata": {
        "id": "mL2hR247xO0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll now pass our extracted feature maps (along with a model_string to save temporary files)\n",
        "# through another convenience function, with parameters for the sparse random projection.\n",
        "# after reducing its dimensionality, we delete the full-sized feature_map with the keep_ argument\n",
        "feature_maps_redux = srp_extraction(model_string, feature_maps = feature_maps, eps = 0.1, seed = 0,\n",
        "                                    delete_original_feature_maps = True, upsampling=False)"
      ],
      "metadata": {
        "id": "G3_b3dLaxRlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here are the dimensionalities of our reduced feature_maps\n",
        "for feature_map in feature_maps_redux:\n",
        "  print(feature_map, feature_maps_redux[feature_map].shape)"
      ],
      "metadata": {
        "id": "nMjhUtdCFSxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having extracted our target features and reduced their dimensionality, we now move to the bulk of the analysis: the mapping of deep net features to the brain data. In this example, we consider two mapping metrics: classic representational similarity analysis and reweighted (voxel-wise encoding) representational similarity analysis.\n",
        "\n",
        "The first of these methods involves calculating RDMs (built with the 1 - Pearson distance) directly from each feature space, then comparing these directly to the target RDMs with a second-order (1 - Pearson) distance metric.\n",
        "\n",
        "The second of these methods involves first fitting an encoding model per voxel per feature space -- effectively regressing the features of a given model layer onto the activation profile of a target voxel. With these encoding models, we can generate predicted activity per voxel, and use this predicted activity to build predicted (voxel-reweighted) RDMs, which we subsequently compare with tthe second (1 - Pearson) distance metric to the relevant brain RDMs.\n",
        "\n",
        "These two metric are comparable, but make different assumptions about the nature of the information that undergirds the similarity between brain and machine. Keep this in mind if comparing the two side by side!"
      ],
      "metadata": {
        "id": "c0McfDedEwOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tools for mapping deep nets to brains\n",
        "# and also for some pretty plotting\n",
        "sys.path.append('model_opts')\n",
        "from mapping_methods import *\n",
        "from ridge_gcv_mod import RidgeCVMod\n",
        "\n",
        "from plotnine import *\n",
        "from plotnine import options\n",
        "options.figure_size = (10,5)"
      ],
      "metadata": {
        "id": "wLm40qjHKGl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here, we define some convenience functions for splitting our data into train and test sets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def get_splithalf_xy(feature_map, response_data, scaling = StandardScaler()):\n",
        "    data_splits = {'train': {}, 'test': {}}\n",
        "\n",
        "    data_splits['train']['X'] = scaling.fit_transform(feature_map[::2,:])\n",
        "    data_splits['test']['X'] = scaling.transform(feature_map[1::2,:])\n",
        "\n",
        "    response_data = response_data.to_numpy()\n",
        "    data_splits['train']['y'] = response_data[:,::2]\n",
        "    data_splits['test']['y'] = response_data[:,1::2]\n",
        "\n",
        "    return data_splits\n",
        "\n",
        "def get_splithalf_rdms(rdms):\n",
        "    split_rdms = {}\n",
        "    for roi_name in rdms:\n",
        "        split_rdms[roi_name] = {}\n",
        "        for subj_id in rdms[roi_name]:\n",
        "            split_rdms[roi_name][subj_id] = {'train': rdms[roi_name][subj_id][::2,::2],\n",
        "                                            'test': rdms[roi_name][subj_id][1::2,1::2]}\n",
        "\n",
        "    return split_rdms"
      ],
      "metadata": {
        "id": "mY6ZMpFdBlFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run this for RidgeGCV Code\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model._ridge import LinearModel, MultiOutputMixin, RegressorMixin\n",
        "from sklearn.linear_model._ridge import _RidgeGCV, _BaseRidgeCV, RidgeCV\n",
        "from sklearn.linear_model._ridge import is_classifier, check_scoring, _check_gcv_mode\n",
        "from sklearn.linear_model._ridge import _IdentityRegressor, safe_sparse_dot\n",
        "\n",
        "from sklearn.linear_model._base import _preprocess_data, _rescale_data\n",
        "\n",
        "from sklearn.metrics import r2_score, explained_variance_score\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "pearsonr_vec = np.vectorize(pearsonr, signature='(n),(n)->(),()')\n",
        "\n",
        "def pearson_r_score(y_true, y_pred, multioutput=None):\n",
        "    y_true_ = y_true.transpose()\n",
        "    y_pred_ = y_pred.transpose()\n",
        "    return(pearsonr_vec(y_true_, y_pred_)[0])\n",
        "\n",
        "class _RidgeGCVMod(_RidgeGCV):\n",
        "    \"\"\"Ridge regression with built-in Leave-one-out Cross-Validation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        alphas=(0.1, 1.0, 10.0),\n",
        "        *,\n",
        "        fit_intercept=True,\n",
        "        scoring=None,\n",
        "        copy_X=True,\n",
        "        gcv_mode=None,\n",
        "        store_cv_values=False,\n",
        "        is_clf=False,\n",
        "        alpha_per_target=False,\n",
        "    ):\n",
        "        self.alphas = np.asarray(alphas)\n",
        "        self.fit_intercept = fit_intercept\n",
        "        self.scoring = scoring\n",
        "        self.copy_X = copy_X\n",
        "        self.gcv_mode = gcv_mode\n",
        "        self.store_cv_values = store_cv_values\n",
        "        self.is_clf = is_clf\n",
        "        self.alpha_per_target = alpha_per_target\n",
        "\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        _normalize = False\n",
        "\n",
        "        X, y = self._validate_data(\n",
        "            X,\n",
        "            y,\n",
        "            accept_sparse=[\"csr\", \"csc\", \"coo\"],\n",
        "            dtype=[np.float64],\n",
        "            multi_output=True,\n",
        "            y_numeric=True,\n",
        "        )\n",
        "\n",
        "        # alpha_per_target cannot be used in classifier mode. All subclasses\n",
        "        # of _RidgeGCV that are classifiers keep alpha_per_target at its\n",
        "        # default value: False, so the condition below should never happen.\n",
        "        assert not (self.is_clf and self.alpha_per_target)\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            sample_weight = _check_sample_weight(sample_weight, X, dtype=X.dtype)\n",
        "\n",
        "        if np.any(self.alphas <= 0):\n",
        "            raise ValueError(\n",
        "                \"alphas must be strictly positive. Got {} containing some \"\n",
        "                \"negative or null value instead.\".format(self.alphas)\n",
        "            )\n",
        "\n",
        "        X, y, X_offset, y_offset, X_scale = _preprocess_data(\n",
        "            X,\n",
        "            y,\n",
        "            self.fit_intercept,\n",
        "            _normalize,\n",
        "            self.copy_X,\n",
        "            sample_weight=sample_weight,\n",
        "        )\n",
        "\n",
        "        gcv_mode = _check_gcv_mode(X, self.gcv_mode)\n",
        "\n",
        "        if gcv_mode == \"eigen\":\n",
        "            decompose = self._eigen_decompose_gram\n",
        "            solve = self._solve_eigen_gram\n",
        "        elif gcv_mode == \"svd\":\n",
        "            if sparse.issparse(X):\n",
        "                decompose = self._eigen_decompose_covariance\n",
        "                solve = self._solve_eigen_covariance\n",
        "            else:\n",
        "                decompose = self._svd_decompose_design_matrix\n",
        "                solve = self._solve_svd_design_matrix\n",
        "\n",
        "        n_samples = X.shape[0]\n",
        "\n",
        "        if sample_weight is not None:\n",
        "            X, y = _rescale_data(X, y, sample_weight)\n",
        "            sqrt_sw = np.sqrt(sample_weight)\n",
        "        else:\n",
        "            sqrt_sw = np.ones(n_samples, dtype=X.dtype)\n",
        "\n",
        "        X_mean, *decomposition = decompose(X, y, sqrt_sw)\n",
        "\n",
        "        if self.scoring not in ['pearson_r', 'explained_variance']:\n",
        "            raise ValueError(\"modified RidgeCV scoring requires one of ['pearson_r','explained_variance']\")\n",
        "\n",
        "        n_y = 1 if len(y.shape) == 1 else y.shape[1]\n",
        "        n_alphas = 1 if np.ndim(self.alphas) == 0 else len(self.alphas)\n",
        "\n",
        "        if self.store_cv_values:\n",
        "            self.cv_values_ = np.empty((n_samples * n_y, n_alphas), dtype=X.dtype)\n",
        "\n",
        "        best_coef, best_score, best_alpha = None, None, None\n",
        "\n",
        "        for i, alpha in enumerate(np.atleast_1d(self.alphas)):\n",
        "            G_inverse_diag, c = solve(float(alpha), y, sqrt_sw, X_mean, *decomposition)\n",
        "            predictions = y - (c / G_inverse_diag)\n",
        "            if self.store_cv_values:\n",
        "                self.cv_values_[:, i] = predictions.ravel()\n",
        "\n",
        "            identity_estimator = _IdentityRegressor()\n",
        "            if self.alpha_per_target:\n",
        "                if self.scoring == 'pearson_r':\n",
        "                    alpha_score = pearson_r_score(y, predictions)\n",
        "                if self.scoring == 'explained_variance':\n",
        "                    alpha_score = explained_variance_score(y, predictions, multioutput = 'raw_values')\n",
        "            else:\n",
        "                if self.scoring == 'pearson_r':\n",
        "                    alpha_score = pearson_r_score(y, predictions).mean()\n",
        "                if self.scoring == 'explained_variance':\n",
        "                    alpha_score = explained_variance_score(y, predictions, multioutput = 'uniform_average')\n",
        "\n",
        "            # Keep track of the best model\n",
        "            if best_score is None:\n",
        "                if self.alpha_per_target and n_y > 1:\n",
        "                    best_coef = c\n",
        "                    best_score = np.atleast_1d(alpha_score)\n",
        "                    best_alpha = np.full(n_y, alpha)\n",
        "                else:\n",
        "                    best_coef = c\n",
        "                    best_score = alpha_score\n",
        "                    best_alpha = alpha\n",
        "            else:\n",
        "                if self.alpha_per_target and n_y > 1:\n",
        "                    to_update = alpha_score > best_score\n",
        "                    best_coef[:, to_update] = c[:, to_update]\n",
        "                    best_score[to_update] = alpha_score[to_update]\n",
        "                    best_alpha[to_update] = alpha\n",
        "                elif alpha_score > best_score:\n",
        "                    best_coef, best_score, best_alpha = c, alpha_score, alpha\n",
        "\n",
        "        self.alpha_ = best_alpha\n",
        "        self.best_score_ = best_score\n",
        "        self.dual_coef_ = best_coef\n",
        "        self.coef_ = safe_sparse_dot(self.dual_coef_.T, X)\n",
        "\n",
        "        X_offset += X_mean * X_scale\n",
        "        self._set_intercept(X_offset, y_offset, X_scale)\n",
        "\n",
        "        if self.store_cv_values:\n",
        "            if len(y.shape) == 1:\n",
        "                cv_values_shape = n_samples, n_alphas\n",
        "            else:\n",
        "                cv_values_shape = n_samples, n_y, n_alphas\n",
        "            self.cv_values_ = self.cv_values_.reshape(cv_values_shape)\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "class _BaseRidgeCVMod(_BaseRidgeCV):\n",
        "    def fit(self, X, y, sample_weight=None):\n",
        "        cv = self.cv\n",
        "        if cv is None:\n",
        "            estimator = _RidgeGCVMod(\n",
        "                self.alphas,\n",
        "                fit_intercept=self.fit_intercept,\n",
        "                scoring=self.scoring,\n",
        "                gcv_mode=self.gcv_mode,\n",
        "                store_cv_values=self.store_cv_values,\n",
        "                is_clf=is_classifier(self),\n",
        "                alpha_per_target=self.alpha_per_target,\n",
        "            )\n",
        "            estimator.fit(X, y, sample_weight=sample_weight)\n",
        "            self.alpha_ = estimator.alpha_\n",
        "            self.best_score_ = estimator.best_score_\n",
        "            if self.store_cv_values:\n",
        "                self.cv_values_ = estimator.cv_values_\n",
        "        else:\n",
        "            if self.store_cv_values:\n",
        "                raise ValueError(\"cv!=None and store_cv_values=True are incompatible\")\n",
        "            if self.alpha_per_target:\n",
        "                raise ValueError(\"cv!=None and alpha_per_target=True are incompatible\")\n",
        "            parameters = {\"alpha\": self.alphas}\n",
        "            solver = \"sparse_cg\" if sparse.issparse(X) else \"auto\"\n",
        "            model = RidgeClassifier if is_classifier(self) else Ridge\n",
        "            gs = GridSearchCV(\n",
        "                model(\n",
        "                    fit_intercept=self.fit_intercept,\n",
        "                    solver=solver,\n",
        "                ),\n",
        "                parameters,\n",
        "                cv=cv,\n",
        "                scoring=self.scoring,\n",
        "            )\n",
        "            gs.fit(X, y, sample_weight=sample_weight)\n",
        "            estimator = gs.best_estimator_\n",
        "            self.alpha_ = gs.best_estimator_.alpha\n",
        "            self.best_score_ = gs.best_score_\n",
        "\n",
        "        self.coef_ = estimator.coef_\n",
        "        self.intercept_ = estimator.intercept_\n",
        "        self.n_features_in_ = estimator.n_features_in_\n",
        "        if hasattr(estimator, \"feature_names_in_\"):\n",
        "            self.feature_names_in_ = estimator.feature_names_in_\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "class RidgeCVMod(MultiOutputMixin, RegressorMixin, _BaseRidgeCVMod):\n",
        "    \"\"\"Ridge regression with built-in cross-validation.\"\"\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7_wofj3DNBxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define our main function here; we'll use this to get results\n",
        "def get_benchmarking_results(feature_maps_redux, model_option):\n",
        "\n",
        "  # information about the model to save in our scoresheets\n",
        "  model_name = model_option['model_name']\n",
        "  train_type = model_option['train_type']\n",
        "\n",
        "  # here, we begin the scoring procedure, first defining which metrics we want to save\n",
        "  scoresheet_lists = {metric: [] for metric in ['crsa','srpr','wrsa']}\n",
        "\n",
        "  #we now loop through all our target_layers, scoring each as we go\n",
        "  for model_layer_index, model_layer in enumerate(tqdm(feature_maps_redux, desc = 'Mapping (Layer)')):\n",
        "      xy = get_splithalf_xy(feature_maps_redux[model_layer], benchmark.response_data)\n",
        "\n",
        "      # these are the value of the lambda penalty in the ridge regression\n",
        "      alpha_values = np.logspace(-1,5,7).tolist()\n",
        "\n",
        "      # this is the regression that maps the feature space to the voxels.\n",
        "      # it's a multioutput regression, so it fits all voxels simultaneously.\n",
        "      # it automatically performs leave one out cross-validation ...\n",
        "      # ... to choose the optimal lambda penalty. we save this loocv score\n",
        "      # ... as the predictive score for the training set, but also ...\n",
        "      # ... evaluate the optimal lambda regression on a heldout test set.\n",
        "      regression = RidgeCVMod(alphas=alpha_values, store_cv_values = True,\n",
        "                              alpha_per_target = True, scoring = 'pearson_r')\n",
        "\n",
        "      # here's where we extract the optimal lambda and generate the test predictions\n",
        "      regression.fit(xy['train']['X'], xy['train']['y'].transpose())\n",
        "      best_alpha_idx = np.array([alpha_values.index(alpha_) for alpha_ in regression.alpha_])\n",
        "      predictions = {'train': np.take_along_axis(regression.cv_values_, best_alpha_idx[None,:,None], axis = 2)[:,:,0],\n",
        "                      'test': xy['test']['X'].dot(regression.coef_.transpose()) + regression.intercept_}\n",
        "\n",
        "      for metric in scoresheet_lists:\n",
        "\n",
        "          if metric == 'crsa':\n",
        "              # even though the classic rsa metric is nonparametric\n",
        "              # it's still wise to split into a train / test set\n",
        "              # for the purposes of selecting the maximally correspondent layers\n",
        "              splithalf_rdms = get_splithalf_rdms(benchmark.rdms)\n",
        "              model_rdms = {'train': 1 - np.corrcoef(feature_maps_redux[model_layer][::2,:]),\n",
        "                            'test': 1 - np.corrcoef(feature_maps_redux[model_layer][1::2,:])}\n",
        "\n",
        "              for score_set in ['train', 'test']:\n",
        "                  for roi_name in benchmark.rdms:\n",
        "                      for subj_id in benchmark.rdms[roi_name]:\n",
        "                          target_rdm = splithalf_rdms[roi_name][subj_id][score_set]\n",
        "                          score = compare_rdms(model_rdms[score_set], target_rdm)\n",
        "                          # what we've done in the lines above is loop through all\n",
        "                          # possible [ROI][subj_id] RDMs and compared them to the layer RDM\n",
        "\n",
        "                          # we save this score and other data in a dictionary\n",
        "                          # for later aggregation in a scoresheet\n",
        "                          scoresheet = {'score': score,\n",
        "                                        'score_set': score_set,\n",
        "                                        'roi_name': roi_name,\n",
        "                                        'subj_id': subj_id,\n",
        "                                        'model': model_name,\n",
        "                                        'train_type': train_type,\n",
        "                                        'model_layer': model_layer,\n",
        "                                        'model_layer_index': model_layer_index,\n",
        "                                        'distance_1': 'pearson_r',\n",
        "                                        'distance_2': 'pearson_r'}\n",
        "\n",
        "                          scoresheet_lists['crsa'].append(scoresheet)\n",
        "\n",
        "          if metric == 'srpr':\n",
        "              # we can, if we want, save the predictive scores per voxel\n",
        "              # prior to aggregating the voxelwise predictions in an RDM\n",
        "              for score_set in ['train','test']:\n",
        "                  if score_set == 'train':\n",
        "                      y = xy['train']['y'].transpose()\n",
        "                      y_pred = predictions['train']\n",
        "                  if score_set == 'test':\n",
        "                      y = xy['test']['y'].transpose()\n",
        "                      y_pred = predictions['test']\n",
        "                  score = score_func(y, y_pred)\n",
        "\n",
        "                  neuroid_ids = benchmark.response_data.index\n",
        "                  # notice in this dictionary that we're saving a score per voxel\n",
        "                  # individual units we occasionally called 'neuroids',\n",
        "                  # to have a general term across different brain units\n",
        "                  scoresheet = pd.DataFrame({benchmark.index_name: neuroid_ids,\n",
        "                                              'score': score,\n",
        "                                              'alpha': regression.alpha_.mean(),\n",
        "                                              'score_set': score_set,\n",
        "                                              'score_type': 'pearson_r',\n",
        "                                              'model': model_name,\n",
        "                                              'train_type': train_type,\n",
        "                                              'model_layer': model_layer,\n",
        "                                              'model_layer_index': model_layer_index})\n",
        "\n",
        "\n",
        "                  scoresheet_lists['srpr'].append(scoresheet)\n",
        "\n",
        "          if metric == 'wrsa':\n",
        "              # now we perform the weighted rsa\n",
        "              splifhalf_rdms = get_splithalf_rdms(benchmark.rdms)\n",
        "              # the rdm_indices tell us which predictions\n",
        "              # from the regression correspond to which voxel\n",
        "              rdm_indices_dict = benchmark.rdm_indices\n",
        "\n",
        "              for score_set in ['train', 'test']:\n",
        "                  for roi_name in benchmark.rdms:\n",
        "                      for subj_id in benchmark.rdms[roi_name]:\n",
        "                          # again we iterate through all [ROI][subj_id] combos ...\n",
        "                          # aggregating the responses from the fitted regression\n",
        "                          # into the relevant predicted RDMs we compare to actual RDMS\n",
        "                          rdm_indices = rdm_indices_dict[roi_name][subj_id]\n",
        "                          prediction_subset = predictions[score_set][:,rdm_indices]\n",
        "                          model_rdm = 1 - np.corrcoef(prediction_subset)\n",
        "                          target_rdm = splithalf_rdms[roi_name][subj_id][score_set]\n",
        "                          score = compare_rdms(model_rdm, target_rdm)\n",
        "\n",
        "                          # this scoresheet should be identical in its dimensionality\n",
        "                          # and composition to the crsa scoresheet\n",
        "                          scoresheet = {'score': score,\n",
        "                                        'score_set': score_set,\n",
        "                                        'alpha': regression.alpha_.mean(),\n",
        "                                        'roi_name': roi_name,\n",
        "                                        'subj_id': subj_id,\n",
        "                                        'model': model_name,\n",
        "                                        'train_type': train_type,\n",
        "                                        'model_layer': model_layer,\n",
        "                                        'model_layer_index': model_layer_index,\n",
        "                                        'distance_1': 'pearson_r',\n",
        "                                        'distance_2': 'pearson_r'}\n",
        "\n",
        "                          scoresheet_lists['wrsa'].append(scoresheet)\n",
        "\n",
        "  # now, we concatenate our various results into easily accessible dataframes\n",
        "  # adding relevant bits of metadata for group_by operations\n",
        "  prime_metadata = benchmark.metadata[['roi_name','subj_id']].reset_index(drop=False)\n",
        "\n",
        "  results = {}\n",
        "\n",
        "  for metric in scoresheet_lists:\n",
        "    if 'rsa' in metric:\n",
        "        results[metric] = pd.DataFrame(scoresheet_lists[metric])\n",
        "    if 'srpr' in metric:\n",
        "        results[metric] = pd.concat(scoresheet_lists[metric]).merge(prime_metadata)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "99tmL-YVAvQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's run the function, and get our results\n",
        "results = get_benchmarking_results(feature_maps_redux, model_option)\n",
        "\n",
        "#del feature_maps_redux # save some space; we won't need these anymore"
      ],
      "metadata": {
        "id": "061M58Q-SB_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fitting our regressions and computing the relevant representational similarities, we can now visualize the scores for individual ROIs across model layer, simultaneously scrutinizing the drop in generalization from training to test set (if in fact there is one)."
      ],
      "metadata": {
        "id": "81xjd1gc9y7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(ggplot(results['crsa'], aes('model_layer_index', 'score', color = 'roi_name', group = 'roi_name')) +\n",
        " geom_point() + geom_line() + facet_wrap('~score_set') + theme_bw())"
      ],
      "metadata": {
        "id": "9oDoTKF1A8iR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(ggplot(results['wrsa'], aes('model_layer_index', 'score', color = 'roi_name', group = 'roi_name')) +\n",
        " geom_point() + geom_line() + facet_wrap('~score_set') + theme_bw())"
      ],
      "metadata": {
        "id": "3mbDIYVYFiTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And there we have it! You've just benchmarked a CLIP model on the NSD data. Obviously, there are many more combinations you could try and different aggregates of the mappings you could scrutinize, but we leave this to you."
      ],
      "metadata": {
        "id": "4kDXe1fX-E6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feeling limited by vision? Try a language model!"
      ],
      "metadata": {
        "id": "6SqCnSSwSYIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for this, we'll need the sentence_transformers package\n",
        "!pip install --quiet sentence_transformers"
      ],
      "metadata": {
        "id": "qpnpfEu7TeUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the following loads an SBERT model to process\n",
        "# the coco captions associated with the images\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "model = model.eval()\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()"
      ],
      "metadata": {
        "id": "BLgY3Z6GSVck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each coco image in the NSD stimulus set has 5 or 6 captions\n",
        "# here, we concatenate them into a lsit\n",
        "all_captions = [eval(caption_list) for caption_list in benchmark.stimulus_data.coco_captions]"
      ],
      "metadata": {
        "id": "PqYuz2avUCqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, we pass them through the SBERT model,\n",
        "# and get an average embedding across the 5 captions per image\n",
        "text_feature_list = []\n",
        "for i in tqdm(range(5)):\n",
        "    captions = [captions_list[i] for captions_list in all_captions]\n",
        "    text_feature_list.append(model.encode(captions))\n",
        "\n",
        "text_features = np.stack(text_feature_list, axis = 0).mean(axis = 0)"
      ],
      "metadata": {
        "id": "s9bs4UW8Srnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then we, can pass these text features through our SRP extractor:\n",
        "feature_maps = get_feature_map_srps({'SBert-Output': text_features})"
      ],
      "metadata": {
        "id": "kWS07V81SnET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this gives us an output of the following dimensions\n",
        "feature_maps['SBert-Output'].shape"
      ],
      "metadata": {
        "id": "xU4EuVhSUKib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here, since we don't have SBERT instrumentalized\n",
        "# we make a simple metadata dict for our benchmarking\n",
        "model_option = {'model_name': 'SBert',\n",
        "                'train_type': 'MaskedLM'}\n",
        "\n",
        "results = get_benchmarking_results(feature_maps, model_option)"
      ],
      "metadata": {
        "id": "RpvDLWf9UOiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(ggplot(results['crsa'], aes('roi_name', 'score', color = 'roi_name', group = 'roi_name')) +\n",
        " geom_col() + facet_wrap('~score_set') + theme_bw() + coord_flip())"
      ],
      "metadata": {
        "id": "_6s6gmW-lWYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(ggplot(results['wrsa'], aes('roi_name', 'score', color = 'roi_name', group = 'roi_name')) +\n",
        " geom_col() + facet_wrap('~score_set') + theme_bw() + coord_flip())"
      ],
      "metadata": {
        "id": "DDm_bge4WGPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As is evident, effectively the same process we used above can be easily repurposed for any number of feature spaces: language models included. Interestingly, we see that language embeddings do remarkable well in predicting selective cortex, but quite poorly in early visual cortex..."
      ],
      "metadata": {
        "id": "tcUPeuwGXXWD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Using Deep Dive in Decoding: Object Identity Decoding\n",
        "\n"
      ],
      "metadata": {
        "id": "9Seo3WM8KBMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../DeepDive"
      ],
      "metadata": {
        "id": "JyP_184mQlbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brain activity is of course not the only reasonable target for deep dive paradigms. What if we're interested in behavioral activity, or some kind of psychological benchmark we consider latent to a given network's learned features?\n",
        "\n",
        "As it turns out, we can use effectively the same pipeline we used above to create encoding models of the brain, to create decoding models for any number of psychological phenomena. As a simple case, let's look to the SimpleShapes Dataset, a set of simple geometric objects with randomized texture meant to deisgned for pedagogy's sake and to explore the shape / texture tradeoff in deep net recognition capabilities."
      ],
      "metadata": {
        "id": "zugGoDc6rBS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the next few cells are a series of imports\n",
        "# and a downloading of the image data\n",
        "import os, sys, shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from plotnine import *\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm as tqdm"
      ],
      "metadata": {
        "id": "HRm9vBUIJbJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('deepdive')\n",
        "from model_options import *\n",
        "from feature_extraction import *\n",
        "from feature_reduction import *"
      ],
      "metadata": {
        "id": "W7uB-iBQRyzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=10WzhgUaRRZMQ9rm020ApZHhq6GvNLZoe\n",
        "!tar xfj simple_shapes.bz2"
      ],
      "metadata": {
        "id": "6_7ff-vLKE6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SimpleShapes dataset consists of 6 objects, randomly placed, rotated and texturized."
      ],
      "metadata": {
        "id": "YkfmSUohsrSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = [plt.imread(f\"SimpleShapes/00001{i}.jpg\") for i in ['Cube', 'Sphere', 'Cylinder', 'Cone', 'Torus', 'Suzanne']]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, sharex=True, sharey=True)\n",
        "\n",
        "for img, ax in zip(images, axes.flat):\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')"
      ],
      "metadata": {
        "id": "pEzwgkWAJWs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First things first, we'll need to put our image data in a data frame, which at minimum consists of the path to each stimulus, and (in decoding-type assays) a label for the stimulus. Additionally information, like group-indices, might also be appended."
      ],
      "metadata": {
        "id": "AgCGWTyHs16P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assets = glob('SimpleShapes/*.jpg') # glob all our image files\n",
        "\n",
        "dictlist = [] # running list of rows\n",
        "for asset in assets:\n",
        "\n",
        "    # loop through the filenames and extract\n",
        "    # indices and labels:\n",
        "    imgstr = asset.split('/')[1]\n",
        "    index = imgstr[:5]\n",
        "    item = imgstr[5:imgstr.rfind('.')]\n",
        "\n",
        "    # add these to a dictionary:\n",
        "    row = {'image_path': asset, 'image_name': imgstr,\n",
        "           'image_index': int(index), 'label': item}\n",
        "\n",
        "    # append dictionary to running list of rows\n",
        "    dictlist.append(row)\n",
        "\n",
        "# create a dataframe from our list of\n",
        "label_df = pd.DataFrame(dictlist)\n",
        "label_df = label_df.query('image_index <= 500').reset_index()"
      ],
      "metadata": {
        "id": "m6o3ehh8JjXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we've aggregated together our stimulus data, we need to split it somehow into a training and testing set, to make sure any decoding model we fit can properly generalize to unseen stimuli.\n",
        "\n",
        "Here's a train-test-split we perform manually to ensure equal numbers of objects in each category."
      ],
      "metadata": {
        "id": "CHcw6jXRtEnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# save all our target dataset sizes\n",
        "train_size, eval_size = 400, 100\n",
        "\n",
        "# split the indices in our dataset into training and test\n",
        "all_indices = list(range(1, train_size + eval_size))\n",
        "test_indices = train_test_split(all_indices, test_size = eval_size)[1]\n",
        "label_df['test_set'] = pd.to_numeric(label_df['image_index']).isin(test_indices)\n",
        "\n",
        "# confirm our splitting produced even quantities\n",
        "print('Proportion of Labels in Training / Eval')\n",
        "print(label_df.groupby(['test_set'])['label'].value_counts())"
      ],
      "metadata": {
        "id": "E4XmHYMlO7kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we begin the feature extraction. As above, we'll first load and instrumentalize our model, installing whatever packages we need to load it."
      ],
      "metadata": {
        "id": "6QejBD9UtY6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('model_opts')\n",
        "from feature_extraction import *"
      ],
      "metadata": {
        "id": "APlpU7b4NRZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "M0KZewjvNnK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_string = 'ViT-B/32_clip'\n",
        "model_option = get_model_options()[model_string]\n",
        "model_name = model_option['model_name']\n",
        "train_type = model_option['train_type']\n",
        "\n",
        "image_transforms = get_recommended_transforms(model_string)\n",
        "model = eval(model_option['call'])\n",
        "model = model.eval()\n",
        "if torch.cuda.is_available():\n",
        "  model = model.cuda()"
      ],
      "metadata": {
        "id": "NX9fenZNM7md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll create a dataloader to feed both our images (train and test) into our feature extraction algorithm. **Importantly!** We must again ensure that our dataloader does not scramble the order of images, which we must keep to link them again to the metadata after feature extraction."
      ],
      "metadata": {
        "id": "0qmcWElgtgk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stimulus_loader = DataLoader(StimulusSet(label_df.image_path, image_transforms), batch_size = 64)\n",
        "get_dataloader_sample(stimulus_loader, nrow = 8)"
      ],
      "metadata": {
        "id": "Ax0NRC55N11F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because again, we're RAM-limited, we'll only select a subset of the layers: Linear layers 6,12,18, and 24 -- for a nice subsample of the hierarchy."
      ],
      "metadata": {
        "id": "SgCIVFeGt2KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_layers = get_empty_feature_maps(model, stimulus_loader, names_only = True)\n",
        "target_layers = [layer for layer in model_layers if 'Linear' in layer\n",
        "                 and int(layer.split('-')[1]) % 6 == 0]"
      ],
      "metadata": {
        "id": "FhsYXVn-NtZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we extract the features, immediately reducing their dimensionality to make them more manageable."
      ],
      "metadata": {
        "id": "xLlD7kJJuAoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps = get_all_feature_maps(model, stimulus_loader, remove_duplicates = False,\n",
        "                                    layers_to_retain = target_layers)"
      ],
      "metadata": {
        "id": "sAtA15PsQMXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature_map in feature_maps:\n",
        "  print(feature_map, feature_maps[feature_map].shape)"
      ],
      "metadata": {
        "id": "PXRZHJgEU1Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_maps_redux = srp_extraction(model_string, feature_maps = feature_maps, eps = 0.1, seed = 0,\n",
        "                                    delete_original_feature_maps = True)"
      ],
      "metadata": {
        "id": "WorC7FszQcoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that done, we need a couple more infrastructural functions that will facilitate the matching of image and label, and the scoring of the decoding model. Our main functions in this will be *prepare_xy* for splitting our image data and features according to our predefined splits, and *parse_performance*, which will facilitate scoring.\n",
        "\n"
      ],
      "metadata": {
        "id": "ywr8Pl1BuI5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import scale\n",
        "\n",
        "def prepare_xy(image_df, predictors, label_col = 'Label', test_set_col = 'TestSet', scale_x = False):\n",
        "    y, y_code = pd.factorize(image_df[label_col])\n",
        "    X = predictors if not scale_x else scale(predictors)\n",
        "\n",
        "    y_test = y[image_df[test_set_col]]\n",
        "    X_test = X[image_df[test_set_col]]\n",
        "    y_train = y[~image_df[test_set_col]]\n",
        "    X_train = X[~image_df[test_set_col]]\n",
        "\n",
        "    return(X_train, y_train, X_test, y_test)"
      ],
      "metadata": {
        "id": "uTPa60LSZR-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_row(x):\n",
        "    return 1 if x['response'] == x['groundtruth'] else 0\n",
        "\n",
        "def parse_performance(image_df, y, y_pred, label_col = 'label', test_set_col = 'test_set'):\n",
        "    new_image_df = image_df.query('{} == True'.format(test_set_col)).copy()\n",
        "    _, y_code = pd.factorize(image_df[label_col])\n",
        "    new_image_df['groundtruth'] = y_code[y]\n",
        "    new_image_df['response'] = y_code[y_pred]\n",
        "    new_image_df['correct'] = new_image_df.apply(lambda x: score_row(x), axis = 1)\n",
        "    new_image_df = new_image_df.drop(['index',test_set_col], axis = 1)\n",
        "\n",
        "    return(new_image_df)"
      ],
      "metadata": {
        "id": "DaIVYNsSZ9x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same regression we used above, but this time as a generalized linear model with categorical outputs (multinomial classifier) rather than a standard linear model with continuous outputs."
      ],
      "metadata": {
        "id": "l3BMEspJutbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifierCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "scoresheet_list = [] # we'll store our performance metrics per layer in this list\n",
        "for model_layer_index, model_layer in enumerate(tqdm(feature_maps_redux, desc = 'Mapping (Layer)')):\n",
        "    X_train, y_train, X_test, y_test = prepare_xy(label_df, feature_maps_redux[model_layer],\n",
        "                                                  label_col = 'label', test_set_col = 'test_set')\n",
        "\n",
        "    alpha_values = np.logspace(-1,5,7).tolist()\n",
        "\n",
        "    # notice, we're using the same optimized ridge regression as above, but\n",
        "    # this time, we're using it as a multinomial generalized linear model\n",
        "    regression = RidgeClassifierCV(alphas=alpha_values).fit(X_train, y_train)\n",
        "    y_pred = regression.predict(X_test)\n",
        "    performance = parse_performance(label_df, y_test, y_pred,\n",
        "                                    label_col = 'label', test_set_col = 'test_set')\n",
        "\n",
        "    performance.insert(0,'model_layer_index', model_layer_index + 1)\n",
        "    performance.insert(0,'model_layer', model_layer)\n",
        "\n",
        "    # append our performance register to a running list\n",
        "    scoresheet_list.append(performance)\n",
        "\n",
        "scoresheet = pd.concat(scoresheet_list)"
      ],
      "metadata": {
        "id": "OfhDyeQMUV3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fit success! How did we do? Below is what we call a \"classification report\" and contains a variety of signal detection metrics both for individual class and overall accuracy."
      ],
      "metadata": {
        "id": "6XSFLxfuvOOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_scoresheet = scoresheet.query('model_layer_index == 1')\n",
        "print(classification_report(target_scoresheet['groundtruth'], target_scoresheet['response']))"
      ],
      "metadata": {
        "id": "Kd0mHmOicM3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can scrutinize performance across layers."
      ],
      "metadata": {
        "id": "Vk9cR1pcvhBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data = scoresheet.groupby(['model_layer_index','label'])['correct'].mean().reset_index()\n",
        "(ggplot(plot_data, aes('model_layer_index', 'correct', color = 'label')) +\n",
        " geom_point() + geom_line(aes(group = 'label')) +  ylim([0,1.0]) + theme_bw() +\n",
        " stat_summary(fun_y = np.mean, geom = 'point', color = 'black') +\n",
        " stat_summary(fun_y = np.mean, geom = 'line', color = 'black', size = 2.5))"
      ],
      "metadata": {
        "id": "Xds4IbkRcz0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And voil times two. You've now fit a deep net decoding model. Happy benchmarking!"
      ],
      "metadata": {
        "id": "vX_yscQDvjY4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd9vyENcvsIg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}